llava-critic
![image](https://github.com/user-attachments/assets/aa418a90-fa93-4277-b84a-66b50e7a7d29)
![image](https://github.com/user-attachments/assets/0f22de04-a338-426c-a360-4a3f77de131a)
![image](https://github.com/user-attachments/assets/4b4693f0-4cc7-4afa-a630-c71444d43639)
![image](https://github.com/user-attachments/assets/8ffb43b8-9aef-49f3-8568-3016cec9930e)
![image](https://github.com/user-attachments/assets/ec5ec791-bb79-4e00-82ea-ed42f4a8b670)
![image](https://github.com/user-attachments/assets/27866702-d786-404b-a531-966598636e79)


![image](https://github.com/user-attachments/assets/ab55050e-e03f-4669-964e-41b3399c24d6)
onevision
![image](https://github.com/user-attachments/assets/137e87cc-89c0-433c-9f60-a05a679c9076)
sima
![image](https://github.com/user-attachments/assets/4fa6290a-1c1f-4f27-9577-7a6dbbc5a530)
archon critic 
![image](https://github.com/user-attachments/assets/782aee3c-d059-49b3-b229-c71fb9669cbc)
Fuser Prompt: Without and With Critiques
![image](https://github.com/user-attachments/assets/c79bb5dd-48e7-4b77-9707-131e4a1c37e2)
decoder based ranking
![image](https://github.com/user-attachments/assets/eea78e63-fa9b-4209-afb6-a7409104b144)
![image](https://github.com/user-attachments/assets/9831a813-1cf8-4a88-ac0f-2156a01da491)
![image](https://github.com/user-attachments/assets/03fa0e1d-d302-4345-8912-9d97d4e89203)
MJBench
![image](https://github.com/user-attachments/assets/47b62778-05b8-4a45-94be-ae0ee42602be)
![image](https://github.com/user-attachments/assets/87a7688a-6537-4363-be4c-895ef7423cda)
![image](https://github.com/user-attachments/assets/79162231-1cf0-4e1b-956a-bdb87c197784)
![image](https://github.com/user-attachments/assets/27eb59b2-fa68-4a2e-8cac-782668b18f63)
![image](https://github.com/user-attachments/assets/84e6830b-00e1-40d4-a7dd-97bb80bce169)
![image](https://github.com/user-attachments/assets/a55f134e-6e83-4724-8c29-ddd628dacb66)
![image](https://github.com/user-attachments/assets/1cb0b4c0-9afa-4a73-a99c-d1a33e8a2c57)

MLLM-as-a-judge score base
![image](https://github.com/user-attachments/assets/68bab37a-1b0b-4986-b56c-7703d97bd05c)

![image](https://github.com/user-attachments/assets/57504c97-e586-4863-b516-d1b5be1e8c0d)
![image](https://github.com/user-attachments/assets/336b2316-7cd1-4263-8959-947a17671764)

![image](https://github.com/user-attachments/assets/34227585-405b-4820-949f-9e7ab37edb7c)

![image](https://github.com/user-attachments/assets/0151ce0b-0104-47fd-9853-ac75a375e22d)
![image](https://github.com/user-attachments/assets/57f910f9-7516-4669-9c76-72d33534c7e5)

![image](https://github.com/user-attachments/assets/3a9a2910-7de6-4362-bf63-34b2d5251b41)

![image](https://github.com/user-attachments/assets/bc51b701-b2f2-4cef-a192-4bb5a60de1b5)
![image](https://github.com/user-attachments/assets/dad8c062-901b-4c9a-bc91-7daf9de3753e)
![image](https://github.com/user-attachments/assets/4ff738d7-3e59-40ae-8446-284c607f1f9a)

LMM-as-a-judge 프로메테우스
-----------
Prompt for rubric augmentation
You are helpful and creative rubric generator. You should brainstorm creative and
impressive three rubrics used to evaluate
the ability of a vision-language model to
generate text when given an image.
The rubric must be structured to assess areas that can be answered by viewing
the image. It consists of a description
explaining specific tasks and criteria for
scoring. Here you will see 4 examples
of ’criteria’, and their scoring rubrics,
formatted as JSON.
Criteria 1:
{Example Criteria 1}
Criteria 2:
{Example Criteria 2}
Criteria 3:
{Example Criteria 3}
Criteria 4:
{Example Criteria 4}
Please brainstorm new three criterias
and scoring rubrics.
Be creative and create new but useful
criteria that people in different settings or
industries might find practical.
Please format the output as same as the
above examples with no extra or surrounding text. And you should not mention
the term like ‘Criteria X:’ and “‘json”’.
In JSON, all keys and string values must
be enclosed in double quotes (""). For
example, "key": "value" is a valid format,
but key: "value" or ’key’: ’value’ are not
valid.
You should create a diverse rubrics suitable
for the given image
Generated criteria:


![image](https://github.com/user-attachments/assets/e96a88d2-bb50-49c4-a4c0-ee7695eeea50)
![image](https://github.com/user-attachments/assets/c0c05903-540d-4c6c-bfb1-cecf5c98bcda)

![image](https://github.com/user-attachments/assets/894085a5-142e-4e77-83e2-bbdf866d4f9d)
![image](https://github.com/user-attachments/assets/094aa94b-9eca-4fe2-b353-cbca28e00a6e)
![image](https://github.com/user-attachments/assets/a824a061-de56-4460-9c51-ca56717077c5)
![image](https://github.com/user-attachments/assets/1e0d5b5c-08d0-4aef-9dda-ae0da7c7fd19)
![image](https://github.com/user-attachments/assets/fe17613e-1dab-415a-9628-a699157f33fd)

![image](https://github.com/user-attachments/assets/baae2c6a-c724-4df9-bdc1-b600761ddc3c)
![image](https://github.com/user-attachments/assets/81178514-fbec-47cb-a72f-303388e48818)
![image](https://github.com/user-attachments/assets/f4343879-7a73-449a-a624-7f69adc2339f)
![image](https://github.com/user-attachments/assets/7de4f4f7-5b11-475a-a727-7ee772e8153c)
![image](https://github.com/user-attachments/assets/8c2d37da-b73a-47ee-8774-cc1b520c41ed)
![image](https://github.com/user-attachments/assets/75520fae-0832-4e33-b0a3-cef8a98bfe3e)

generative reward model
-------------------------
![image](https://github.com/user-attachments/assets/94f61b75-6018-40fe-bc56-f84f834eb469)

Figure 3: An example where LLM-as-a-judge fails to provide an accurate judgement, but STaRDPO succeeds. Sentences where critical reasoning takes place are bolded for emphasis.
![image](https://github.com/user-attachments/assets/adbaf367-b12b-45ef-81b5-8eb53acac845)
![image](https://github.com/user-attachments/assets/41ff60d6-34dd-4bd7-9ecd-7dcc4275450e)
![image](https://github.com/user-attachments/assets/bfa49a63-3afe-4bed-8629-f17367cc9ac2)

critique-out reward model
![image](https://github.com/user-attachments/assets/b9f5518d-63da-4605-a21f-17c5a6c1343c)

![image](https://github.com/user-attachments/assets/9c280996-baec-4984-89bd-2de4fc717bed)
![image](https://github.com/user-attachments/assets/e58b2d64-5b5e-44ef-86c2-5634acbb53dd)

![image](https://github.com/user-attachments/assets/72bee3a5-8cf9-4298-b802-3b188eb7c019)
![image](https://github.com/user-attachments/assets/ef29d777-86fb-4ee9-8827-1d7005597179)

improve vlm cot
-------------------
![image](https://github.com/user-attachments/assets/05cd9809-792c-42d0-b11f-36a76778929a)
![image](https://github.com/user-attachments/assets/b1f10484-5200-45ae-984f-12d8a0a22e0b)

![image](https://github.com/user-attachments/assets/2713b5ee-c227-4654-9de2-a891db191f39)
![image](https://github.com/user-attachments/assets/a292ad40-7ea2-43cf-9651-c40d5c9325dc)
![image](https://github.com/user-attachments/assets/dbc83773-4b96-48c1-bcce-267dc12e662f)

MAVIS-Caption: 588K high-quality caption-diagram pairs, spanning geometry and function
![image](https://github.com/user-attachments/assets/05bd0d5c-f21f-442a-962f-e5d011fadf94)
MAVIS-Instruct: 834K instruction-tuning data with CoT rationales in a text-lite version
![image](https://github.com/user-attachments/assets/2e876c13-e184-46a7-aa13-fe275b6f83b5)
MAVIS-7B: an MLLM with a three-stage training paradigm achiving leading performance on MathVerse benchmark
![image](https://github.com/user-attachments/assets/ed4e8851-2330-480b-acc3-35edb1dd2eb7)

Inferaligner
-------------------
![image](https://github.com/user-attachments/assets/323cdcfa-d5ee-4e06-aa9b-353d64be3f72)
![image](https://github.com/user-attachments/assets/3bea8562-b818-45a9-8e66-6fac4076972b)
